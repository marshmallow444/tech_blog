---
layout: post
title: "【ラビット・チャレンジ】深層学習 前編"
tags: ラビット・チャレンジ E資格 機械学習
---

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax:{inlineMath:[['\$','\$'],['\\(','\\)']],processEscapes:true},CommonHTML: {matchFontHeight:false}});</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

[ラビット・チャレンジ](https://ai999.careers/rabbit/)の受講レポート。  

---  

## プロローグ

### 識別と生成

| | 識別モデル<br>(discriminative, backward) | 生成モデル<br>(generative, forward) |
| --- | --- | --- |
| 目的 | データを目的のクラスに分類する | 特定のクラスのデータを生成する |
| 計算結果 | $p(C_k \| x)$ | $p(x \| C_k)$ |
| 具体的なモデル | 決定木<br>ロジスティック回帰<br>SVM<br>NN | HMM<br>ベイジアンネットワーク<br>VAE<br>GAN |
|  特徴  | 高次元→低次元<br>必要な学習データ：少 |  低次元→高次元<br>必要な学習データ：多 |

### 識別機(Clasifier)の開発アプローチ 

| | 生成モデル | 識別モデル | 識別関数 |
| --- | --- | --- | --- |
| 識別の計算 | $p(x \| C_k) \cdot p(c_k)$を推定<br>ベイズの定理より<br>$p(C_k \| x) =  \frac{p(x \| C_k) \cdot p(C_k)}{p(x)}$<br>ただし$p(x) = \sum_{k}p(x \| C_k) p(C_k)$ | $p(C_k \| x)$を推定<br>決定理論に基づき識別結果を得る<br>(閾値に基づく決定など) | 入力値$x$を直接クラスに写像(変換)する関数$f(x)$を推定 |
| モデル化の対象 | 各クラスの生起確率<br>データのクラス条件付き密度 | データがクラスに属する確率 | データの属するクラスの情報のみ<br>確率は計算されない |
| 特徴 | データを人工的に生成できる<br>確率的な識別 | 確率的な識別 | 学習量が少ない<br>決定的な識別 |


参考：「パターン認識と機械学習」(2007年)  
→生成モデルの研究が発達する前の分類方法  

### 識別器における生成モデルと識別モデル

+ 生成モデル
    + データのクラス条件付き密度の分布を推定
        + 分類結果より複雑、計算量が多い
+ 識別モデル
    + 直接データがクラスに属する確率を求める

### 識別器における識別モデルと識別関数

+ 識別モデル
    + 入力データから事後確率を推論して識別結果を決定
    + 識別結果の確率が得られる
+ 識別関数
    + 識別結果のみ得られる

### 万能近似定理と深さ

活性化関数をもつネットワークを使うことで、どんな関数でも近似できるのでは？という定理  

<br>

---

## Day1: NN

### ニューラルネットワークの全体像

入力層→中間層→出力層  

#### 確認テスト1

+ ディープラーニングは何をしようとしているか？
    + 自分の解答
        + 人間が具体的な数学モデルを直接構築するのではなく、ニューラルネットワークが入力データから特徴量を抽出することで、数学モデルを構築する
    + 模範解答
        + 明示的なプログラムの代わりに多数の中間層を持つニューラルネットワークを用いて、入力値から目的とする出力値に変換するを数学モデルを構築すること。
+ どの値の最適化が最終目標か？
    + 重み(W), バイアス(b)

#### 確認テスト2

次のネットワークを描く  

+ 入力層：2ノード1層
+ 中間層：3ノード2層
+ 出力層：1ノード1層

![test_2]({{site.baseurl}}/images/20211025.drawio.png)  

+ NNの対象とする問題の種類
    + 回帰
    + 分類

### 入力層〜中間層

$u = Wx + b$

1次関数において、  
+ $W$: 傾き
+ $b$: 切片 bias

$W, b$を学習する  

#### 確認テスト3  

![test_3]({{site.baseurl}}/images/20211025_1.png)  

#### 確認テスト

+ 以下の数式をPythonで書く  
    $u = Wx + b$

```
u = np.dot(x, W) + b
```

+ 中間層の出力を定義しているソース

```
# 2層の総入力
u2 = np.dot(z1, W2) + b2

# 2層の総出力
z2 = functions.relu(u2)
```

### 活性化関数

次の層への出力の大きさを決める<u>非線形の関数</u>  
次の層への信号のON/OFFや強弱を定める  

#### 確認テスト  

線形と非線形の違い  

+ 線形：直線  
    + 加法性と斉次性を満たす
+ 非線形：非直線  
    + 加法性と斉次性を満たさない

#### 中間層用の活性化関数

+ ReLU関数
    + 勾配消失問題の回避とスパース化
        + スパース化するとモデルの中身がシンプルになる
+ シグモイド(ロジスティック)関数
    + 勾配消失問題
+ ステップ関数
    + 線形分離可能なものしか学習できない

出力層用の活性化関数

+ ソフトマックス関数
+ 恒等写像
+ シグモイド関数

